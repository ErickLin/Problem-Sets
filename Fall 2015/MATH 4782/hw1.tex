\documentclass[a4paper,12pt]{article}

\usepackage[margin=3.5cm]{geometry}
\usepackage{amsfonts, amsmath, enumitem, physics}

\begin{document}
	
\section*{MATH/PHYS 4782 - HW1 Solutions}

\begin{enumerate}

	\item[2.5.]
        \begin{enumerate}[label=(\arabic*)]
            \item
                \begin{align*}
                    ((y_1, \cdots, y_n), (\lambda_1 z_1, \cdots, \lambda_n z_n)) &= \sum_{i = 1}^{n} y_i^* (\lambda_i z_i) \\
                    &= \sum_{i = 1}^{n} \lambda_i ((y_1, \cdots, y_n), (0, \cdots, 0, z_i, 0, \cdots, 0))
                \end{align*}

            \item
                \begin{align*}
                    ((z_1, \cdots, z_n), (y_1, \cdots, y_n))^* &= \left( \sum_{i = 1}^{n} z_i^* y_i \right)^* \\
                    &= \sum_{i = 1}^{n} y_i^* z_i \\
                    &= ((y_1, \cdots, y_n), (z_i, \cdots, z_n))
                \end{align*}

            \item
                \begin{align*}
                    ((y_1, \cdots, y_n), (y_1, \cdots, y_n)) &= \sum_{i = 1}^{n} y_i^* y_i
                \end{align*}
                and since $y_i = a + bi$ and $y_i^* = a - bi$ for some $a, b \in \mathbb{R}$, $y_i^* y_i = a^2 - b^2 i^2 = a^2 + b^2 \geq 0$, with equality if and only if $a = 0$ and $b = 0$ (or $y_i = 0$). This means that the sum is nonnegative as well, and the zero condition is met if and only if $(y_1, \cdots, y_n) = \vec{0}$.
        \end{enumerate}
	
	\item[2.8.] Since $\{ \ket{w_1}, \cdots, \ket{w_d} \}$ form a linearly independent set, $\ket{w_i} \neq \vec{0}$ so $\ket{v_1} \equiv \frac{\ket{w_1}}{\Vert \ket{w_1} \Vert}$ must equal $1$. Therefore, $\{ \ket{v_1} \}$ forms an orthonormal, linearly independent set of dimension $1$, and for $c \in \mathbb{C}$, its span is given by
	\[ S(\ket{v_1}) = c \ket{v_1} = \frac{c}{\Vert \ket{w_1} \Vert} \ket{w_1} = S(\ket{w_1}). \]
	Now assume that $\ket{v_1}, \cdots, \ket{v_k}$ are orthonormal and that
    \begin{align*}
        S(\ket{v_1}, \cdots, \ket{v_k}) = S(\ket{w_1}, \cdots, \ket{w_k}).
    \end{align*}
    Due to linear independence,
	\[ \ket{w_{k + 1}} - \sum_{i = 1}^{k} c_i \ket{w_i}, c_i \in \mathbb{C} \]
	must be nonzero, and so from the linear span assumption
	\[ \ket{w_{k + 1}} - \sum_{i = 1}^{k} c_i \ket{v_i}, c_i \in \mathbb{C} \]
	must be nonzero as well. Then from the definition, $\ket{v_{k + 1}} = 1$ and $\ket{w_{k + 1}} \in S(\ket{v_1}, \cdots, \ket{v_{k + 1}})$. Combined with the linear span assumption, the definition also shows that $\ket{v_{k + 1}} \in S(\ket{w_1}, \cdots, \ket{w_{k + 1}})$, with the result that
	\[ S(\ket{v_1}, \cdots, \ket{v_{k + 1}}) = S(\ket{w_1}, \cdots, \ket{w_{k + 1}}). \] Now for any $v_m$ with $m \leq k$,
	\begin{align*}
		\braket{v_m}{v_{k + 1}} &\propto \braket{v_m}{\ket{w_{k + 1}} - \textstyle{ \sum_{i = 1}^{k} \textstyle} \braket{v_i}{w_{k + 1}} \ket{v_i}} \\
		&= \braket{v_m}{w_{k + 1}} - \textstyle{ \sum_{i = 1}^{k} \textstyle} \braket{v_i}{w_{k + 1}} \braket{v_m}{v_i} \\
		&= \braket{v_m}{w_{k + 1}} - \braket{v_m}{w_{k + 1}} \\
		&= 0
	\end{align*}
	which means that $\ket{v_1}, \cdots, \ket{v_{k + 1}}$ are orthonormal, completing the induction. In particular, $\ket{v_1}, \cdots, \ket{v_d}$ are orthonormal and $S(\ket{v_1}, \cdots, \ket{v_d}) \\ = S(\ket{w_1}, \cdots, \ket{w_d})$. Because $\{ \ket{w_1}, \cdots, \ket{w_d} \}$ forms a basis for $V$, $\{ \ket{v_1}, \cdots, \ket{v_d} \}$ therefore forms an orthonormal basis for $V$.
	
	\item[2.9.]
	\begin{align*}
		I
		&= \left[ \begin{array}{cc}
		1 & 0 \\
		0 & 0
		\end{array} \right]
		+ \left[ \begin{array}{cc}
		0 & 0 \\
		0 & 1
		\end{array} \right]
		= \ket{0}\bra{0} + \ket{1}\bra{1}
		\\
		X
		&= \left[ \begin{array}{cc}
		0 & 1 \\
		0 & 0
		\end{array} \right]
		+ \left[ \begin{array}{cc}
		0 & 0 \\
		1 & 0
		\end{array} \right]
		= \ket{0}\bra{1} + \ket{1}\bra{0}
		\\
		Y
		&= \left[ \begin{array}{cc}
		0 & 0 \\
		i & 0
		\end{array} \right]
		+ \left[ \begin{array}{cc}
		0 & -i \\
		0 & 0
		\end{array} \right]
		= i\ket{1}\bra{0} - i\ket{0}\bra{1}
		\\
		Z
		&= \left[ \begin{array}{cc}
		1 & 0 \\
		0 & 0
		\end{array} \right]
		+ \left[ \begin{array}{cc}
		0 & 0 \\
		0 & -1
		\end{array} \right]
		= \ket{0}\bra{0} - \ket{1}\bra{1}
	\end{align*}
	
	\item[2.17.] $(\Rightarrow)$ From the definition of Hermitian matrices, $A = A^\dagger$. If $\lambda$ is an eigenvalue of $A$, then there exist vectors $\vec{v}$ such that
	\begin{align*}
		A \ket{v} &= \lambda \ket{v} \\
		(A \ket{v})^\dagger &= (\lambda \ket{v})^\dagger \\
		\ket{v}^\dagger A^\dagger &= \lambda^* \ket{v}^\dagger \\
		\ket{v}^\dagger A^\dagger \ket{v} &= \lambda^* \ket{v}^\dagger \ket{v} \\
		\lambda \ket{v}^\dagger \ket{v} &= \lambda^* \ket{v}^\dagger \ket{v} \\
		\lambda &= \lambda^*,
	\end{align*}
	which is true only when $\lambda \in \mathbb{R}$. \par
	$(\Leftarrow)$ Because normal matrices are diagonalizable, $A$ can be written as $S \Lambda S^\dagger$, where $\Lambda$ is the diagonal matrix containing the eigenvalues of $A$. Since the eigenvalues are real, $\Lambda = \Lambda^\dagger$. Then
	\begin{align*}
		A^\dagger &= (S \Lambda S^\dagger)^\dagger \\
		&= (S^\dagger)^\dagger \Lambda^\dagger S^\dagger \\
		&= S \Lambda S^\dagger \\
		&= A.
	\end{align*}
	
	\item[2.20.] Let $B = [ \ket{v_1} \cdots \ket{v_d} ]$ and $C = [ \ket{w_1} \cdots \ket{w_d} ]$ where $d = \mbox{dim}(V)$. $B$ and $C$ convert vectors from their corresponding bases into the standard basis (used by $A$) and are invertible because a basis forms an independent set, while $B^{-1}$ and $C^{-1}$ convert the vectors back into their original bases. Then
	\begin{align}
		A' &= B^{-1} A B \\
		A'' &= C^{-1} A C
	\end{align}
	Multiplying both sides of $(1)$,
	\begin{align}
		A = B A' B^{-1}
	\end{align}
	and substituting $(3)$ into $(2)$,
	\[ A'' = C^{-1} B A' B^{-1} C. \]
	
	\item[2.27.]
	\begin{align*}
		X \otimes Z
		&= \left[ \begin{array}{cc}
		0Z & 1Z \\
		1Z & 0Z
		\end{array} \right]
		= \left[ \begin{array}{cccc}
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & -1 \\
		1 & 0 & 0 & 0 \\
		0 & -1 & 0 & 0
		\end{array} \right]
		\\
		X \otimes I
		&= \left[ \begin{array}{cc}
		0I & 1I \\
		1I & 0I
		\end{array} \right]
		= \left[ \begin{array}{cccc}
		0 & 0 & 1 & 0 \\
		0 & 0 & 0 & 1 \\
		1 & 0 & 0 & 0 \\
		0 & 1 & 0 & 0
		\end{array} \right]
		\\
		I \otimes X
		&= \left[ \begin{array}{cc}
		1X & 0X \\
		0X & 1X
		\end{array} \right]
		= \left[ \begin{array}{cccc}
		0 & 1 & 0 & 0 \\
		1 & 0 & 0 & 0 \\
		0 & 0 & 0 & 1 \\
		0 & 0 & 1 & 0
		\end{array} \right]
	\end{align*}
	$X \otimes I \neq I \otimes X$ which demonstrates the non-commutativity of the tensor product.
	
	\item[2.33.] Let $x_1, y_1 \in \{0, 1\}$. Then $H$ may be rewritten as
	\[ H = \frac{1}{\sqrt{2}} \sum_{x_1, y_1} (-1)^{x_1 \cdot y_1} \ket{x_1} \bra{y_1}. \]
	Because $H^{\otimes n} = H^{\otimes (n - 1)} \otimes H$,
	\begin{align*}
		H^{\otimes n} &= \bigotimes_{i = 1}^{n} \left( \frac{1}{\sqrt{2}} \sum_{x_i, y_i} (-1)^{x_i \cdot y_i} \ket{x_i} \bra{y_i} \right) \\
		&= \frac{1}{\sqrt{2^n}} \otimes_{i = 1}^{n} \sum_{x_i, y_i} (-1)^{x_i \cdot y_i} \ket{x_i} \bra{y_i}.
	\end{align*}
	If we let $\ket{x} = \otimes_{i = 1}^{n} \ket{x_i}$, $\ket{y} = \otimes_{i = 1}^{n} \ket{y_i}$, and $x \cdot y = \sum_{i = 1}^{n} x_i \cdot y_i$ for all combinations of $x_i$ and $y_i$, then $H^{\otimes n}$ can be rewritten as
	\[ H^{\otimes n} = \frac{1}{\sqrt{2^n}} \sum_{x, y} (-1)^{x \cdot y} \ket{x} \bra{y}. \]
	The matrix representation for $H$ is
	\[
        \frac{1}{\sqrt{2}}
        \left[ \begin{array}{cc}
        1 & 1 \\
        1 & -1
        \end{array} \right]
	\]
	and the matrix representation for $H^{\otimes 2}$ is
	\[
        \frac{1}{\sqrt{2}}
        \left[ \begin{array}{cc}
        1 & 1 \\
        1 & -1
        \end{array} \right]
        \otimes \frac{1}{\sqrt{2}}
        \left[ \begin{array}{cc}
        1 & 1 \\
        1 & -1
        \end{array} \right]
        = \frac{1}{2}
        \left[ \begin{array}{cccc}
        1 & 1 & 1 & 1 \\
        1 & -1 & 1 & -1 \\
        1 & 1 & -1 & -1 \\
        1 & -1 & -1 & 1
        \end{array} \right].
	\]
	
	\item[2.35.] Writing the Taylor series expansion and splitting up the summation,
	\begin{align*}
		\mbox{exp}(i \theta \vec{v} \cdot \vec{\sigma}) &= \sum_{k = 0}^{\infty} \frac{(i \theta \vec{v} \cdot \vec{\sigma})^k}{k!} \\
		&= \sum_{k = 0}^{\infty} \frac{(i \theta \vec{v} \cdot \vec{\sigma})^{2k}}{(2k)!} + \sum_{k = 0}^{\infty} \frac{(i \theta \vec{v} \cdot \vec{\sigma})^{2k + 1}}{(2k + 1)!} \\
		&= \sum_{k = 0}^{\infty} \frac{(-1)^k \theta^{2k} (\vec{v} \cdot \vec{\sigma})^{2k}}{(2k)!} + \sum_{k = 0}^{\infty} \frac{i (-1)^k \theta^{2k + 1} (\vec{v} \cdot \vec{\sigma})^{2k + 1}}{(2k + 1)!}
	\end{align*}
	One property of the Pauli matrices is
	\begin{alignat*}{4}
        \sigma_i \sigma_j &= - &\sigma&_j \sigma_i, & \; i \neq j \\
        \sigma_i \sigma_j &=   &  I   &,            & \; i = j
	\end{alignat*}
	which leads to the result that
	\begin{align*}
		(\vec{v} \cdot \vec{\sigma})^{2k} &= \left( \sum_{i = 1}^{3} v_i \sigma_i \right)^{2k} \\
		&= \left( \sum_{i = 1}^{3} \sum_{j = 1}^{3} v_i v_j \sigma_i \sigma_j \right)^k \\
		&= \left( \sum_{i = 1}^{3} v_i^2 I \right)^k \\
		&= I^k \\
		&= I
	\end{align*}
	since $\vec{v}$ is a unit vector. Then we have that
	\begin{align*}
		\mbox{exp}(i \theta \vec{v} \cdot \vec{\sigma}) &= \sum_{k = 0}^{\infty} \frac{(-1)^k \theta^{2k}}{(2k)!} I + i \sum_{k = 0}^{\infty} \frac{(-1)^k \theta^{2k + 1}}{(2k + 1)!} \vec{v} \cdot \vec{\sigma} \\
		&= \cos(\theta) I + i \sin(\theta) \vec{v} \cdot \vec{\sigma}.
	\end{align*}

\end{enumerate}

\end{document}
