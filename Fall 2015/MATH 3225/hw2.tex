\documentclass[a4paper,12pt]{article}
\usepackage{amsfonts, amsmath}
\begin{document}
\section*{MATH 3225 - HW2 Solutions}
\begin{enumerate}
    \item
        Using the law of the subconscious statistician and the definition of the Poisson distribution,
        \begin{align*}
            \mathbb{E}\left(X(X - 1)(X - 2)\cdots(X - k)\right) &= \sum_{m = k + 1}^{\infty} m(m - 1)(m - 2)\cdots(m - k) \left( \frac{\lambda^m e^{-\lambda}}{m!} \right) \\
            &= \sum_{m = k + 1}^{\infty} \frac{\lambda^{m - k - 1}}{(m - k - 1)!} \lambda^{k + 1} e^{-\lambda} \\
            &= e^\lambda \lambda^{k + 1} e^{-\lambda} \\
            &= \lambda^{k + 1}
        \end{align*}

    \setcounter{enumi}{2}
    \item
        Using the law of the subconscious statistician, we have
        \begin{align*}
            \mathbb{E} (X^2) = \sum_{x \in \text{Im}X} x^2 \mathbb{P}(X = x) = 0.
        \end{align*}
        Because each term of the summation is nonnegative ($x^2 \geq 0$ by the Trivial Inequality, and $\mathbb{P}(X = x) \geq 0$ due to the definition of a probability mass function), the terms must all be zero in order for the sum to be zero. It can be deduced then that if $\mathbb{P}(X = x) > 0$, it must be that $x^2 = 0$ so $x = 0$. Because $\sum_{x \in \text{Im}X} \mathbb{P}(X = x) = 1$ by definition, the only positive term $\mathbb{P}(X = 0)$ must equal $1$. \par
        Writing out the definition of $\text{var}(X)$,
        \begin{align*}
            \text{var}(X) = \mathbb{E}([X - \mu]^2) &= 0 \\
            \sum_{x \in \text{Im}X} (x - \mu)^2 \mathbb{P}(X = (x - \mu)^2) &= 0 \\
        \end{align*}
        Applying the previous proposition, we have $x - \mu = 0 \Rightarrow x = \mu$.

    \item
        One of the properties that must be satisfied is that
        \[
            \sum_{k = 1}^{\infty} p(k) = \sum_{k = 1}^{\infty} ck^\alpha = 1
        \]
        which yields
        \[
            \sum_{k = 1}^{\infty} k^\alpha = \frac{1}{c}.
        \]
        As a $p$-series, the summation above converges only for $\alpha < - 1$. If this is the case, then $c = 1 / \sum_{k = 1}^\infty k^\alpha$.

    \setcounter{enumi}{5}
    \item
        \begin{align*}
            \mathbb{E}(X) &= \sum_{k = 0}^\infty k\mathbb{P}(N = k) \\
            &= [\mathbb{P}(N = 1) + 2\mathbb{P}(N = 2) + \cdots] + [2\mathbb{P}(N = 2) + 3\mathbb{P}(N = 3) + \cdots] + \cdots \\
            &= \sum_{k = 0}^\infty \sum_{l = k + 1}^\infty \mathbb{P}(N = l) \\
            &= \sum_{k = 0}^\infty \mathbb{P}(N > k)
        \end{align*}
        Let $R$ be the event in which red does not appear in $k$ trials, and $B$ and $G$ be the analogous events for blue and green. Then $\mathbb{P}(R \cap B)$ is the event that neither red nor blue appear in $k$ trials, and so on. Computing these probabilities,
        \begin{gather*}
            \mathbb{P}(R) = \mathbb{P}(B) = \mathbb{P}(G) = \left( \frac{2}{3} \right)^k \\
            \mathbb{P}(R \cap B) = \mathbb{P}(R \cap G) = \mathbb{P}(B \cap G) = \left( \frac{1}{3} \right)^k \\
            \mathbb{P}(R \cap B \cap G) = 0
        \end{gather*}
        If $k = 0$, then the probability that not all colors appear in $k$ trials is $1$. Otherwise, from the Inclusion-Exclusion Principle,
        \begin{align*}
            \mathbb{P}(R \cup B \cup G) &= \mathbb{P}(R) + \mathbb{P}(B) + \mathbb{P}(G) - \mathbb{P}(R \cap B) - \mathbb{P}(R \cap G) - \mathbb{P}(B \cap G) \\
            &\phantom{{}=1} + \mathbb{P}(R \cap B \cap G) \\
            &= 3\left( \frac{2}{3} \right)^k - 3\left( \frac{1}{3} \right)^k + 0 \\
            &= \frac{2^k - 1}{3^{k - 1}}.
        \end{align*}
        Finally, for the defined random variable $N$, $N > n$ can be interpreted as the event in which all three colors appear only for all values greater than $n$. This is equivalent to the event in which not all three colors appear in $n$ throws. Then
        \begin{align*}
            \mathbb{E}(N) &= \sum_{n = 0}^\infty \mathbb{P}(N > n) \\
            &= 1 + \sum_{n = 1}^\infty \frac{2^n - 1}{3^{n - 1}} \\
            &= 1 + 2\sum_{n = 1}^\infty \left( \frac{2}{3} \right)^{n - 1} - \sum_{n = 1}^\infty \left( \frac{1}{3} \right)^{n - 1} \\
            &= 1 + 2\left( \frac{1}{1 - \frac{2}{3}} \right) - \frac{1}{1 - \frac{1}{3}} \\
            &= \frac{11}{2}
        \end{align*}

    \setcounter{enumi}{8}
    \item The probability for the event that the first $n$ tosses are all heads is $p^n$, and the contribution of this event to the expected value $\mathbb{E}(X)$ is $np^n$. Otherwise, a tail appears after $i < n$ heads occur in a row, and the contribution of each of these events to $\mathbb{E}(X)$ is $p^i (1 - p) [\mathbb{E}(X) + i + 1]$. Then
        \begin{align*}
            \mathbb{E}(X) &= np^n + \sum_{i = 0}^{n - 1} p^i (1 - p) [\mathbb{E}(X) + i + 1] \\
            &= np^n + \left[ \mathbb{E}(X) (1 - p) \sum_{i = 0}^{n - 1} p^i \right] + \left[ (1 - p) \sum_{i = 1}^{n - 1} ip^i \right] + \left[ (1 - p) \sum_{i = 0}^{n - 1} p^i \right] \\
            &= np^n + [\mathbb{E}(X) (1 - p^n)] + \left[ \left( \sum_{i = 1}^{n - 1} [ip^i - (i - 1)p^i ] \right) - (n - 1)p^n \right] + [1 - p^n] \\
            &= \mathbb{E}(X) (1 - p^n) + \sum_{i = 1}^{n - 1} p^i + 1 \\
            &= \mathbb{E}(X) (1 - p^n) + \sum_{i = 0}^{n - 1} p^i \\
            &= \frac{\sum_{i = 0}^{n - 1} p^i}{p^n} \\
            &= \sum_{k = 1}^{n} p^{-k}
        \end{align*}

\end{enumerate}
\end{document}
